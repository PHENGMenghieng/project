import cv2
import mediapipe as mp
import joblib
import numpy as np

model = joblib.load("hand_model.pkl")
mp_hands = mp.solutions.hands

cap = cv2.VideoCapture(0)

with mp_hands.Hands(max_num_hands=1) as hands:
    while True:
        ret, frame = cap.read()
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = hands.process(rgb)

        if result.multi_hand_landmarks:
            lm = result.multi_hand_landmarks[0].landmark
            row = []
            for pt in lm:
                row.extend([pt.x, pt.y, pt.z])

            X = np.array(row).reshape(1, -1)
            pred = model.predict(X)[0]

            cv2.putText(frame, f"Gesture: {pred}", (30, 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 3)

        cv2.imshow("Hand Gesture Recognition", frame)
        if cv2.waitKey(1) == 27:
            break

cap.release()
cv2.destroyAllWindows()
